{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from scipy.spatial import distance\n",
    "from n2i.__main__ import n2i_main, n2i_nx_graph\n",
    "from n2i.node2vec import read_graph\n",
    "\n",
    "%pylab inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_path = '../data/graph/lesmiserables/lesmiserables_edgelist.txt'\n",
    "\n",
    "graph = read_graph(weighted=False, \n",
    "               graph=graph_path,\n",
    "               directed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "emb = n2i_nx_graph(nx_graph=graph, topics=3, tf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity between connected nodes\n",
    "def sim_in(G):\n",
    "    sims = []\n",
    "    for i in G.nodes:\n",
    "        for j in list(G.neighbors(i)):\n",
    "            sims.append(1 - distance.cosine(G.nodes[i]['interests'], G.nodes[j]['interests']))\n",
    "    return np.mean(sims)\n",
    "\n",
    "def select_notedge(G):\n",
    "    v1 = np.random.choice(G.nodes())\n",
    "    v2 = np.random.choice(G.nodes())\n",
    "\n",
    "    while (v1,v2) in G.edges or v1==v2:\n",
    "        v1 = np.random.choice(G.nodes())\n",
    "        v2 = np.random.choice(G.nodes())\n",
    "    return v1, v2\n",
    "#     n = nx.number_of_nodes(G)\n",
    "#     while True:\n",
    "#         a, b = np.random.randint(0, n, size=2)\n",
    "#         if (a, b) not in G.edges:\n",
    "#             return a, b\n",
    "\n",
    "# similarity between disconnected nodes\n",
    "def sim_out(G, samples):\n",
    "    sims_out = []\n",
    "    for c in range(samples):\n",
    "        i, j = select_notedge(G)\n",
    "        sims_out.append(1 - distance.cosine(G.nodes[i]['interests'], G.nodes[j]['interests']))\n",
    "    return np.mean(sims_out)\n",
    "\n",
    "def homophily(G):\n",
    "    return sim_in(G) / sim_out(G, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = '../data/graph/lesmiserables/lesmiserables_edgelist.txt'\n",
    "G = read_graph(weighted=False, \n",
    "               graph=graph,\n",
    "               directed=False)\n",
    "p_val = [0.1, 0.25, 0.5, 0.75, 1.25, 1.5, 1.75, 2, 4, 8, 10, 20, 40]\n",
    "q_val = [0.1, 0.25, 0.5, 0.75, 1.25, 1.5, 1.75, 2, 4, 8, 10, 20, 40]\n",
    "\n",
    "topics=[15]\n",
    "dimensions=[128]\n",
    "walk_length=[80]\n",
    "num_walks=[10]\n",
    "window_size=[10]\n",
    "iiter=[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_list = []\n",
    "\n",
    "nr_experiments = 20\n",
    "\n",
    "for t in topics:\n",
    "    for d in dimensions:\n",
    "        for wk in walk_length:\n",
    "            for n in num_walks:\n",
    "                for wi in window_size:\n",
    "                    for ii in iiter:\n",
    "                        for p in p_val:\n",
    "                            for q in q_val:\n",
    "                                for seed in range(nr_experiments):\n",
    "                                    args = [t, d, wk, n, wi, ii, p, q, seed]\n",
    "                                    args_list.append(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(*args):\n",
    "    t, d, wk, n, wi, ii, p, q, seed = args\n",
    "    G_emb = n2i_nx_graph(nx_graph=G, topics=t, \n",
    "             dimensions=d, walk_length=wk,\n",
    "             num_walks=n, window_size=wi,\n",
    "             iiter=ii, p=p, q=q,\n",
    "             seed = seed + int(1000*(q+p)))\n",
    "    for i in G.nodes:\n",
    "        G.node[i]['interests'] = G_emb[i]\n",
    "    si = sim_in(G)\n",
    "    so = sim_out(G, 5000)\n",
    "    return args + (si/so,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = list(map(lambda x: run_experiment(*x), args_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(result, columns=['t', 'd', 'wk', 'n', 'wi', 'ii', 'p', 'q', 'seed', 'hom'])\n",
    "df2 = df.groupby(['p', 'q'])['hom'].mean().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc()[[2420]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df2)\n",
    "plt.title('Homophily (mean of 20 realizations)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(graph, weighted, directed):\n",
    "    '''\n",
    "    Pipeline for the heatmap creation\n",
    "    '''\n",
    "    # read graph\n",
    "    G = graph\n",
    "    # experiments\n",
    "    result = list(map(lambda x: run_experiment(*x), args_list))\n",
    "    # df\n",
    "    df = pd.DataFrame(result, columns=['t', 'd', 'wk', 'n', 'wi', 'ii', 'p', 'q', 'seed', 'hom'])\n",
    "    df2 = df.groupby(['p', 'q'])['hom'].mean().unstack()\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.karate_club_graph()\n",
    "weighted =  False if graph.edge_attr_dict_factory() == {} else True\n",
    "directed = graph.is_directed()\n",
    "df = analysis(graph, weighted, directed)\n",
    "\n",
    "# plot\n",
    "sns.heatmap(df)\n",
    "plt.title('Homophily (mean of 20 realizations)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing score in link prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "batch_size  100 num_skips  25\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-681a8a6d5895>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                    \u001b[0mwalk_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                    \u001b[0mnum_walks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                    dimensions=20)\n\u001b[0m",
      "\u001b[0;32m~/Work/Progetti/WoMG/WoMG/node2interests/n2i/__main__.py\u001b[0m in \u001b[0;36mn2i_nx_graph\u001b[0;34m(nx_graph, topics, weighted, directed, fast, seed, dimensions, walk_length, num_walks, window_size, iiter, workers, p, q, tf, normalize, translate, reduce, verbose)\u001b[0m\n\u001b[1;32m     43\u001b[0m                                 \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                                 \u001b[0miiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miiter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                                 verbose=verbose)\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/Progetti/WoMG/WoMG/node2interests/n2i/node2vec.py\u001b[0m in \u001b[0;36mnode2vec\u001b[0;34m(weighted, graph, directed, p, q, num_walks, walk_length, dimensions, window_size, workers, iiter, verbose, tf)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mwalks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulate_walks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_walks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwalk_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     emb_model = learn_embeddings(G.G.number_of_nodes(), walks, dimensions, window_size,\n\u001b[0;32m---> 63\u001b[0;31m                                  workers, iiter, verbose=verbose, use_tf=tf)\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0memb_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/Progetti/WoMG/WoMG/node2interests/n2i/node2vec.py\u001b[0m in \u001b[0;36mlearn_embeddings\u001b[0;34m(number_of_nodes, walks, dimensions, window_size, workers, iiter, use_tf, verbose)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_tf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfWord2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_of_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdimensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwalks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miiter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mwalks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwalk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwalk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwalks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/Progetti/WoMG/WoMG/node2interests/n2i/tfoptimizer.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, walks, batch_size, iiter, num_skips, window, verbose)\u001b[0m\n\u001b[1;32m    129\u001b[0m                                                      \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                                                      \u001b[0mnum_skips\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_skips\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                                                      window=window):\n\u001b[0m\u001b[1;32m    132\u001b[0m                     \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                     _, cur_loss = session.run([\n",
      "\u001b[0;32m~/Work/Progetti/WoMG/WoMG/node2interests/n2i/tfoptimizer.py\u001b[0m in \u001b[0;36mgenerate_batch\u001b[0;34m(walks, batch_size, num_skips, window)\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0mwords_to_use\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_skips\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_word\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_to_use\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                     \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_skips\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                     \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_skips\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontext_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "emb = n2i_nx_graph(nx_graph=graph, topics=12, tf=True, seed=10, translate=False, reduce=False, \n",
    "                   window_size=20,\n",
    "                   walk_length=100,\n",
    "                   num_walks=30,\n",
    "                   dimensions=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for u, v in graph.edges():\n",
    "    arr_u = list(emb[u])\n",
    "    arr_v = list(emb[v])\n",
    "    X.append(arr_u + arr_v)\n",
    "    y.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as IT\n",
    "import random\n",
    "\n",
    "missing = [pair for pair in IT.combinations(graph.nodes(), 2) if not graph.has_edge(*pair)]\n",
    "no_edges = random.choices(missing, k=len(graph.edges()))\n",
    "for u, v in no_edges:\n",
    "    arr_u = list(emb[u])\n",
    "    arr_v = list(emb[v])\n",
    "    X.append(arr_u + arr_v)\n",
    "    y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-vs-rest logistic regression object\n",
    "clf = LogisticRegression(random_state=0, multi_class='ovr')\n",
    "# Train model\n",
    "model = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
