{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *WoMG*: tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a tutorial for *WoMG*. The WoMG software generates synthetic datasets of documents cascades on network. It starts with any (un)directed, (un)weighted graph and a collection of documents and it outputs the propagation DAGs of the docs through the network. Diffusion process is guided by the nodes underlying preferences.\n",
    "\n",
    "\n",
    "First section will introduce a demo of the software.\n",
    "\n",
    "Second section will provide a basic usage with the [Digg](https://www.isi.edu/~lerman/downloads/digg2009.html) dataset used in the thesis. Digg dataset contains a netowkr dataset and a propagation dataset.\n",
    "\n",
    "The Analysis section will analyse the synthetic dataset of propagations generated by WoMG using the Digg network as input graph; the real propagation values are reported at the end of the previous section. \n",
    "\n",
    "The statistics section is used for simulating different propagations with different parameters and plotting the results; this kind of statistics needs lot of computation by WoMG, so synthetic datasets are already produced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOC:\n",
    "* [Demo](#Demo)\n",
    "* [Help](#Help)\n",
    "* [Usage](#Usage)\n",
    "* [Analysis](#Analysis)\n",
    "* [Statistics](#Statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WoMG can be used both in command line and Jupyter notebook. The following codes will run WoMG with the default parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/womg_main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "path = pathlib.Path(\"../src\")\n",
    "sys.path.insert(0, str(path))\n",
    "from womg_main import womg_main\n",
    "\n",
    "womg_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../Output/ ; ls ; #cat Diffusion_formatted_output_sim0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the parameters by the help page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function womg_main in module __main__:\n",
      "\n",
      "womg_main(numb_topics=15, numb_docs=None, numb_steps=100, homophily=0.5, actives_perc=0.05, virality=1, path_in_graph=PosixPath('/Users/Cinus/Work/Progetti/WoMG/WoMG/data/graph/lesmiserables/lesmiserables_edgelist.txt'), weighted=False, directed=False, god_node=False, docs_path=None, path_out=None, fformat='txt', seed=None, dimensions=128, walk_length=80, num_walks=10, window_size=10, iiter=1, workers=8, p=1, q=1)\n",
      "    ----------------------------------------------------------------------------------\n",
      "    WoMG main function: \n",
      "    \n",
      "    The *WoMG* software generates synthetic datasets of documents cascades on network. \n",
      "    It starts with any (un)directed, (un)weighted graph and a collection of documents \n",
      "    and it outputs the propagation DAGs of the docs through the network. \n",
      "    Diffusion process is guided by the nodes underlying preferences. \n",
      "    Please check the github page for more details.\n",
      "    \n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    numb_topics : int\n",
      "        number of topics in the topic model. Default 15. K<d \n",
      "    \n",
      "    numb_docs : int\n",
      "        number of docs to be generated. Default 100\n",
      "    \n",
      "    numb_steps : int\n",
      "        number of time steps for diffusion\n",
      "    \n",
      "    homophily : float\n",
      "        0<=H<=1 :degree of homophily decoded from the given network. \n",
      "        1-H is degree of influence between nodes; \n",
      "        reccommended values are: 0, 0.5, 1. Default 0.5\n",
      "    \n",
      "    actives_perc : float\n",
      "        maximum percentage of active nodes in first step of simulation on an item \n",
      "        with respect to the number of nodes. Default 0.05\n",
      "    \n",
      "    virality : float\n",
      "        exponent of the powerlaw distribution for documents viralities. \n",
      "        P(x; a) = x^{-a}, 0 <= x <=1. Deafault a=1\n",
      "    \n",
      "    \n",
      "    path_in_graph : str\n",
      "        input path of the graph edgelist\n",
      "    \n",
      "    weighted : bool\n",
      "        boolean specifying (un)weighted. Default  unweighted\n",
      "    \n",
      "    directed : bool\n",
      "        graph is (un)directed. Default  undirected\n",
      "    \n",
      "    \n",
      "    docs_path : str\n",
      "        input  path of the documents folder\n",
      "    \n",
      "    path_out : str\n",
      "        outputs path\n",
      "    \n",
      "    fformat : str\n",
      "        file formats. Supported formats are txt and pickle. Default txt\n",
      "    \n",
      "    seed : int\n",
      "        seed (int) for random distribution extractions\n",
      "    \n",
      "    \n",
      "    dimensions : int\n",
      "        [node2vec param] number of dimensions. Default 128\n",
      "    \n",
      "    walk_length : int\n",
      "        [node2vec param] length of walk per source. Default 80\n",
      "    \n",
      "    num_walks : int\n",
      "        [node2vec param] number of walks per source. Default 10\n",
      "    \n",
      "    window_size : int\n",
      "        [node2vec param] context size for optimization. Default 10\n",
      "    \n",
      "    iiter : int\n",
      "        [node2vec param] number of epochs in SGD\n",
      "    \n",
      "    workers: int \n",
      "        [node2vec param] number of parallel workers. Default 8\n",
      "    \n",
      "    p : float\n",
      "        [node2vec param] manually set BFS parameter; else: it is set by H\n",
      "    \n",
      "    q : float\n",
      "        [node2vec param] manually set DFS parameter; else: it is set by H\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(womg_main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First set of quantitative parameters are:\n",
    "\n",
    "1. number of topics to be considered in the topic distributions of documents and nodes interests; it has to be less than number of dimensions of the nodes' space provided by node2vec\n",
    "2. number of documents TO BE GENERATED by lda, giving this parameter lda will be directly set to generative mode\n",
    "3. steps of the diffusion simulation\n",
    "4. H degree of homophily. Node2vec is used as baseline for generating interests vectors of the nodes starting from the given graph. Parameters *p* and *q* can achieve different decoded degree of homophily and structural equivalence (see paper). The best mix of them can be achieved only by a deep analysis of the network and a grid searh on the parameters. In order to pursuit generality in the input graph we use three degree of mixing: structural equivalence predominant, deepWalk (p=1, q=1), homophily predominant (which are not the best for representing the graph!).  1-H is the degree of social influence between nodes; which is the percentage of the avg interests vecs norms to be assigned to the influence vectors.\n",
    "5. percentage of active nodes with respect to the total number of nodes in the intial configuration (before diffusion) for each doc.\n",
    "6. virality of the doc; if virality is high, exponent of the power law is high and threshold for activation is low.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next parameters concern input graph, input documents and the node2vec original parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will produce a synthetic propagation dataset on the [Digg network dataset](https://www.isi.edu/~lerman/downloads/digg2009.html). This dataset consists in: graph dataset and diffusion dataset. We used the first as input of WoMG for generating diffusions and analyse results. \n",
    "\n",
    "We set:\n",
    "\n",
    "1. the number of steps equal to 100\n",
    "2. the maximum percentage of active nodes per doc equal to 0.065\n",
    "3. number of generated docs equal to 3553\n",
    "4. virality exponent of the docs distribution equal to 0.009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 ../src/womg_main.py --graph ../data/graph/digg/digg_edgelist.txt --directed --steps 100 --actives 0.065 --docs 3553  --virality 0.009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis of the actions using digg network as input can be done using simulation_index=_tutorial.\n",
    "\n",
    "The real dataset analysis provides the following results:\n",
    "\n",
    "    items actions [max, min, avg]:   6265 105 505\n",
    "\n",
    "    users actions [max, min, avg]:   3415 20 115"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the synthetic dataset results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pathlib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_index = \"_tutorial\"\n",
    "output_path = pathlib.Path.cwd() / \"Output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_info = output_path / str(\"Network_info_sim\"+str(simulation_index)+\".txt\")\n",
    "file_prop = output_path / str(\"Diffusion_formatted_output_sim\"+str(simulation_index)+\".txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(file_in):\n",
    "    '''\n",
    "    Returns file from the given input path\n",
    "    '''\n",
    "    if str(pathlib.Path(file_in).suffix) == '.txt':\n",
    "        with open(file_in, 'r') as f:\n",
    "            s = f.readlines()\n",
    "    if str(pathlib.Path(file_in).suffix) == '.pickle':\n",
    "        with open(file_in, 'rb') as f:\n",
    "            s = pickle.load(f)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dict(inp, typ=False):\n",
    "    '''\n",
    "    if typ:\n",
    "        Returns info input (inp) in dict format\n",
    "    if not typ:\n",
    "        Returns cascades in a dict format:\n",
    "        (outer)first key: time, (inner)second key: item, \n",
    "        value: new active nodes\n",
    "    '''\n",
    "    \n",
    "    if typ:\n",
    "        info_dict = ast.literal_eval(str(inp).replace('[','').replace(']','').replace('\"',''))\n",
    "        out_dict = info_dict\n",
    "        \n",
    "    else:   \n",
    "        prop_dict = {}\n",
    "        index = 0\n",
    "        for i in range(2, len(prop), 2):\n",
    "            inp[i] = inp[i].replace('\\n', '')\n",
    "            inp[i] = inp[i].replace('set()','None')\n",
    "            prop_dict[index] = ast.literal_eval(prop[i])\n",
    "            index += 1\n",
    "\n",
    "        out_dict = prop_dict\n",
    "           \n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_str = extract(file_info)\n",
    "info = to_dict(info_str, typ=True)\n",
    "prop = extract(file_prop)\n",
    "cascades = to_dict(prop, typ=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### analysis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# items actions\n",
    "def items_actions(cascades, plot=False):\n",
    "    '''\n",
    "    Returns the vector of all items' actions\n",
    "    each entry is the the number of activations(actions)\n",
    "    for the item identified by entry-index\n",
    "    '''\n",
    "    numb_docs = max(cascades[0].keys())\n",
    "    items_action_vec = [0 for i in range(numb_docs+1)]\n",
    "    for step in cascades.keys():\n",
    "        for item in cascades[step].keys():\n",
    "            if cascades[step][item] != None:\n",
    "                items_action_vec[item] += len(cascades[step][item])\n",
    "            #if cascades[step][item] == None:\n",
    "                #print(item)\n",
    "    #print(items_action_vec)\n",
    "    if plot:\n",
    "        plt.hist(items_action_vec)\n",
    "        plt.show()\n",
    "    \n",
    "    print('items actions [max, min, avg]: ', int(max(items_action_vec)), int(min(items_action_vec)), int(np.mean(items_action_vec)))\n",
    "    return items_action_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_data = items_actions(cascades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users actions\n",
    "def users_actions(cascades):\n",
    "    '''\n",
    "    Returns the vector of all users' actions\n",
    "    each entry is the the number of activations(actions)\n",
    "    for the user identified by entry-index\n",
    "    '''\n",
    "    numb_nodes = int(info['numb_nodes'])\n",
    " \n",
    "        \n",
    "    # defining dict\n",
    "    users_actions_dict = {}\n",
    "    for step in cascades.keys():\n",
    "        for item in cascades[step].keys():\n",
    "            if cascades[step][item] != None:\n",
    "                for node in cascades[step][item]:\n",
    "                    users_actions_dict[node] = 0\n",
    "                    \n",
    "    # counting            \n",
    "    for step in cascades.keys():\n",
    "        for item in cascades[step].keys():\n",
    "            if cascades[step][item] != None:\n",
    "                for node in cascades[step][item]:\n",
    "                    users_actions_dict[node] += 1\n",
    "                    \n",
    "    users_actions_vec = [0 for i in range(numb_nodes)]\n",
    "    for key, index in zip(sorted(users_actions_dict.keys()), range(numb_nodes)):\n",
    "        users_actions_vec[index] = users_actions_dict[key]\n",
    "    \n",
    "    print('users actions [max, min, avg]: ',int(max(users_actions_vec)),\n",
    "          int(min(users_actions_vec)), int(np.mean(users_actions_vec)))\n",
    "    return users_actions_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_data = users_actions(cascades)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "path = pathlib.Path(\"../src\")\n",
    "sys.path.insert(0, str(path))\n",
    "from womg_main import womg_main\n",
    "\n",
    "help(womg_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(numb_docs=None, viralities, actives, topics):\n",
    "    '''\n",
    "    Run womg with different sets of parameters\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        numb_docs : int\n",
    "            number of documents to be generated\n",
    "            (Default None: you need to pass a docs folder path)\n",
    "\n",
    "        viralities : iterable/float\n",
    "            array containing viralitiy params\n",
    "\n",
    "        actives : iterable/float\n",
    "            array containing actives_perc params\n",
    "            (percentage of actives nodes for an items in the initial step)\n",
    "\n",
    "        topics : iterable/int\n",
    "            array containing numb_topics params\n",
    "            \n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        See womg reference for more details: help(womg_main)\n",
    "        \n",
    "    '''\n",
    "    for v in viralities:\n",
    "        for a in actives:\n",
    "            for t in topics:      \n",
    "                womg_main(numb_topics=t, actives_perc=a, virality=v, numb_docs=numb_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viralities = [2, 1, 0.5, 0.45, 0.4, 0.35, 0.3, 0.25, 0.2, 0.15, 0.1, 0.05]\n",
    "actives = [0.065]\n",
    "topics = [15]\n",
    "\n",
    "simulate(numb_docs=3553, \n",
    "         viralities=viralities, \n",
    "         actives=actives, \n",
    "         topics=topics\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### virality plot\n",
    "\n",
    "'''\n",
    "file_idx = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "viralities = [2, 1, 0.5, 0.3, 0.5, 0.35, 0.25, 0.2, 0.1]\n",
    "'''\n",
    "'''\n",
    "\n",
    "file_idx = [0, 2, 6,  16]\n",
    "viralities = [2, 0.5, 0.35,  0.11]\n",
    "'''\n",
    "path = './Output/Diffusion_formatted_output_sim'\n",
    "\n",
    "\n",
    "file_idx = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "viralities = [0.5, 0.45, 0.4, 0.38, 0.35, 0.32, 0.3, 0.28, 0.25, 0.22, 0.2, 0.18, 0.15, 0.12]\n",
    "\n",
    "y_item = []\n",
    "x_item = []\n",
    "y_user = []\n",
    "x_user = []\n",
    "cdf_data = {}\n",
    "cdf_data['item'] = {}\n",
    "cdf_data['user'] = {}\n",
    "for i in tqdm(range(len(file_idx))):\n",
    "    # data for cdf plot\n",
    "    cdf_data['item'][i] = []\n",
    "    cdf_data['user'][i] = []\n",
    "    ########################\n",
    "    file = path + str(file_idx[i]) + '.txt'\n",
    "    prop = extract(file)\n",
    "    cascades = to_dict(prop)\n",
    "    items_data = items_actions(cascades, plot=False)\n",
    "    users_data = users_actions(cascades)\n",
    "    for n in range(len(items_data)):\n",
    "        cdf_data['item'][i].append(items_data[n])\n",
    "        y_item.append(items_data[n])\n",
    "        x_item.append(viralities[i])\n",
    "    for n in range(len(users_data)):\n",
    "        cdf_data['user'][i].append(users_data[n])\n",
    "        y_user.append(users_data[n])\n",
    "        x_user.append(viralities[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### violin plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,11))\n",
    "\n",
    "plt.xlabel('virality', fontsize=22)\n",
    "plt.ylabel('items actions', fontsize=22)\n",
    "\n",
    "ax = sns.violinplot(x=x_item, y=y_item)\n",
    "sns.set()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#fig.savefig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### cdf plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,11))\n",
    "\n",
    "\n",
    "file_idx = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "viralities = [0.5, 0.45, 0.4, 0.38, 0.35, 0.32, 0.3, 0.28, 0.25, 0.22, 0.2, 0.18, 0.15, 0.12]\n",
    "\n",
    "\n",
    "cdf_hist = {}\n",
    "cdf_hist['user'] = {}\n",
    "cdf_hist['item'] = {}\n",
    "for i in [2, 9, 13]:\n",
    "    # evaluate the histogram\n",
    "    cumulative = []\n",
    "    values, base = np.histogram(cdf_data['item'][i], bins=50)\n",
    "    cdf_hist['item'][i] = (values, base)\n",
    "    \n",
    "for i in [2, 9, 13]:\n",
    "    #evaluate the cumulative\n",
    "    cumulative = np.cumsum(cdf_hist['item'][i][0])/3553\n",
    "    # plot the cumulative function\n",
    "    plt.plot(base[:-1], cumulative, linewidth=5)\n",
    "    #print(cumulative)\n",
    "    plt.xlabel('actions', fontsize=33)\n",
    "    plt.ylabel('CDF', fontsize=33)\n",
    "    plt.tick_params(labelsize=17)\n",
    "    #plt.yscale('log')\n",
    "    #plt.xscale('log')\n",
    "    plt.locator_params(axis='y', nbins=20)\n",
    "    plt.locator_params(axis='x', nbins=20)\n",
    "    plt.title('items CDF', fontsize=33)\n",
    "    plt.legend(['virality-exp=0.5', 'virality-exp=0.3', 'virality-exp=0.11', 'virality-exp=0.45', 'virality-exp=0.45'], \n",
    "               fontsize=33)\n",
    "plt.show()\n",
    "#fig.savefig()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
